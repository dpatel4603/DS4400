{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name: Dev Patel \n",
        "\n",
        "Course: DS4400 Data Mining and Machine Learning 1\n",
        "\n",
        "Prof: Silvio Amir\n",
        "\n",
        "University: Northeastern University"
      ],
      "id": "9237909dd91c9e3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Problem 5: Gradient Descent for Linear Regression\n",
        "\n",
        "1. Implement gradient descent for training linear regression.\n",
        "2. Vary learning rate $\\alpha \\in \\{0.01, 0.1, 0.5\\}$ and report $\\theta$ after 10, 50, and 100 iterations. Table of MSE and R² on train/test.\n",
        "3. Observations on convergence behavior."
      ],
      "id": "b125218c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-14T00:38:52.282999Z",
          "start_time": "2026-02-14T00:38:50.940785Z"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "execution_count": 1,
      "outputs": [],
      "id": "e3a8fe15"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-14T00:38:53.926542Z",
          "start_time": "2026-02-14T00:38:53.905177Z"
        }
      },
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "exclude_cols = ['id', 'date', 'zipcode', 'price', '', 'Unnamed: 0']\n",
        "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
        "\n",
        "X_train = train_df[feature_cols].values.astype(float)\n",
        "y_train = train_df['price'].values.astype(float)\n",
        "X_test = test_df[feature_cols].values.astype(float)\n",
        "y_test = test_df['price'].values.astype(float)\n",
        "\n",
        "# Standardize features (zero mean, unit variance) for gradient descent stability\n",
        "train_mean = X_train.mean(axis=0)\n",
        "train_std = X_train.std(axis=0)\n",
        "train_std[train_std == 0] = 1  # avoid division by zero\n",
        "\n",
        "X_train_scaled = (X_train - train_mean) / train_std\n",
        "X_test_scaled = (X_test - train_mean) / train_std\n",
        "\n",
        "print(f\"Features: {feature_cols}\")\n",
        "print(f\"X_train: {X_train_scaled.shape}, y_train: {y_train.shape}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
            "X_train: (1000, 17), y_train: (1000,)\n"
          ]
        }
      ],
      "id": "d80fda86"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-14T00:38:57.560401Z",
          "start_time": "2026-02-14T00:38:57.557243Z"
        }
      },
      "source": [
        "def gradient_descent(X, y, alpha, n_iters):\n",
        "    \"\"\"\n",
        "    Gradient descent for linear regression.\n",
        "    \n",
        "    Cost: J(θ) = (1/2n) * Σ(Xθ - y)²\n",
        "    Gradient: ∇J = (1/n) * X^T (Xθ - y)\n",
        "    Update: θ := θ - α * ∇J\n",
        "    \n",
        "    X: (n, p) feature matrix (no intercept column; added inside)\n",
        "    Returns: theta (p+1,), list of theta snapshots at requested iterations\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    X_design = np.column_stack([np.ones(n), X])  # add intercept\n",
        "    p = X_design.shape[1]\n",
        "    theta = np.zeros(p)  # initialize at zero\n",
        "    \n",
        "    for i in range(1, n_iters + 1):\n",
        "        residual = X_design @ theta - y          # (n,)\n",
        "        gradient = (1 / n) * (X_design.T @ residual)  # (p,)\n",
        "        theta = theta - alpha * gradient\n",
        "    \n",
        "    return theta\n",
        "\n",
        "def predict_gd(X, theta):\n",
        "    \"\"\"Predict using theta. X should NOT include intercept.\"\"\"\n",
        "    X_design = np.column_stack([np.ones(len(X)), X])\n",
        "    return X_design @ theta"
      ],
      "execution_count": 3,
      "outputs": [],
      "id": "53cb4f45"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-14T00:39:04.409631Z",
          "start_time": "2026-02-14T00:39:03.513615Z"
        }
      },
      "source": [
        "alphas = [0.01, 0.1, 0.3, 0.5]\n",
        "iterations = [10, 50, 100]\n",
        "\n",
        "rows = []\n",
        "for alpha in alphas:\n",
        "    for n_iter in iterations:\n",
        "        theta = gradient_descent(X_train_scaled, y_train, alpha, n_iter)\n",
        "        \n",
        "        y_train_pred = predict_gd(X_train_scaled, theta)\n",
        "        y_test_pred = predict_gd(X_test_scaled, theta)\n",
        "        \n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_r2 = r2_score(y_train, y_train_pred)\n",
        "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "        test_r2 = r2_score(y_test, y_test_pred)\n",
        "        \n",
        "        rows.append({\n",
        "            'α': alpha,\n",
        "            'Iterations': n_iter,\n",
        "            'Train MSE': train_mse,\n",
        "            'Train R²': train_r2,\n",
        "            'Test MSE': test_mse,\n",
        "            'Test R²': test_r2\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "fmt = results_df.copy()\n",
        "fmt['Train MSE'] = fmt['Train MSE'].map('{:,.0f}'.format)\n",
        "fmt['Test MSE'] = fmt['Test MSE'].map('{:,.0f}'.format)\n",
        "fmt['Train R²'] = fmt['Train R²'].map('{:.4f}'.format)\n",
        "fmt['Test R²'] = fmt['Test R²'].map('{:.4f}'.format)\n",
        "print(\"Gradient Descent Results:\\n\")\n",
        "fmt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient Descent Results:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>α</th>\n",
              "      <th>Iterations</th>\n",
              "      <th>Train MSE</th>\n",
              "      <th>Train R²</th>\n",
              "      <th>Test MSE</th>\n",
              "      <th>Test R²</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>10</td>\n",
              "      <td>294,798,733,591</td>\n",
              "      <td>-1.5604</td>\n",
              "      <td>350,525,097,299</td>\n",
              "      <td>-1.1024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01</td>\n",
              "      <td>50</td>\n",
              "      <td>138,295,915,198</td>\n",
              "      <td>-0.2011</td>\n",
              "      <td>170,376,668,653</td>\n",
              "      <td>-0.0219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.01</td>\n",
              "      <td>100</td>\n",
              "      <td>70,118,986,603</td>\n",
              "      <td>0.3910</td>\n",
              "      <td>97,486,244,759</td>\n",
              "      <td>0.4153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.10</td>\n",
              "      <td>10</td>\n",
              "      <td>66,499,315,474</td>\n",
              "      <td>0.4224</td>\n",
              "      <td>93,559,294,995</td>\n",
              "      <td>0.4388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.10</td>\n",
              "      <td>50</td>\n",
              "      <td>31,578,978,073</td>\n",
              "      <td>0.7257</td>\n",
              "      <td>58,012,316,715</td>\n",
              "      <td>0.6521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.10</td>\n",
              "      <td>100</td>\n",
              "      <td>31,497,692,326</td>\n",
              "      <td>0.7264</td>\n",
              "      <td>57,725,185,719</td>\n",
              "      <td>0.6538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.30</td>\n",
              "      <td>10</td>\n",
              "      <td>31,923,127,584</td>\n",
              "      <td>0.7227</td>\n",
              "      <td>58,544,326,400</td>\n",
              "      <td>0.6489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.30</td>\n",
              "      <td>50</td>\n",
              "      <td>31,487,744,695</td>\n",
              "      <td>0.7265</td>\n",
              "      <td>57,656,471,652</td>\n",
              "      <td>0.6542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.30</td>\n",
              "      <td>100</td>\n",
              "      <td>31,486,174,819</td>\n",
              "      <td>0.7265</td>\n",
              "      <td>57,629,970,173</td>\n",
              "      <td>0.6543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.50</td>\n",
              "      <td>10</td>\n",
              "      <td>611,829,862,759,887</td>\n",
              "      <td>-5312.9211</td>\n",
              "      <td>685,023,079,342,110</td>\n",
              "      <td>-4107.6524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.50</td>\n",
              "      <td>50</td>\n",
              "      <td>16,494,955,142,450,048,314,529,401,536,512</td>\n",
              "      <td>-143263505624901550080.0000</td>\n",
              "      <td>18,420,828,390,359,931,998,442,476,273,664</td>\n",
              "      <td>-110485009051377795072.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.50</td>\n",
              "      <td>100</td>\n",
              "      <td>5,698,751,634,744,989,337,662,728,205,932,475,...</td>\n",
              "      <td>-49495323256631769873223494181542097321984.0000</td>\n",
              "      <td>6,364,111,026,359,996,756,852,195,932,805,655,...</td>\n",
              "      <td>-38170860150856579751327727291176321024000.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       α  Iterations                                          Train MSE  \\\n",
              "0   0.01          10                                    294,798,733,591   \n",
              "1   0.01          50                                    138,295,915,198   \n",
              "2   0.01         100                                     70,118,986,603   \n",
              "3   0.10          10                                     66,499,315,474   \n",
              "4   0.10          50                                     31,578,978,073   \n",
              "5   0.10         100                                     31,497,692,326   \n",
              "6   0.30          10                                     31,923,127,584   \n",
              "7   0.30          50                                     31,487,744,695   \n",
              "8   0.30         100                                     31,486,174,819   \n",
              "9   0.50          10                                611,829,862,759,887   \n",
              "10  0.50          50         16,494,955,142,450,048,314,529,401,536,512   \n",
              "11  0.50         100  5,698,751,634,744,989,337,662,728,205,932,475,...   \n",
              "\n",
              "                                           Train R²  \\\n",
              "0                                           -1.5604   \n",
              "1                                           -0.2011   \n",
              "2                                            0.3910   \n",
              "3                                            0.4224   \n",
              "4                                            0.7257   \n",
              "5                                            0.7264   \n",
              "6                                            0.7227   \n",
              "7                                            0.7265   \n",
              "8                                            0.7265   \n",
              "9                                        -5312.9211   \n",
              "10                      -143263505624901550080.0000   \n",
              "11  -49495323256631769873223494181542097321984.0000   \n",
              "\n",
              "                                             Test MSE  \\\n",
              "0                                     350,525,097,299   \n",
              "1                                     170,376,668,653   \n",
              "2                                      97,486,244,759   \n",
              "3                                      93,559,294,995   \n",
              "4                                      58,012,316,715   \n",
              "5                                      57,725,185,719   \n",
              "6                                      58,544,326,400   \n",
              "7                                      57,656,471,652   \n",
              "8                                      57,629,970,173   \n",
              "9                                 685,023,079,342,110   \n",
              "10         18,420,828,390,359,931,998,442,476,273,664   \n",
              "11  6,364,111,026,359,996,756,852,195,932,805,655,...   \n",
              "\n",
              "                                            Test R²  \n",
              "0                                           -1.1024  \n",
              "1                                           -0.0219  \n",
              "2                                            0.4153  \n",
              "3                                            0.4388  \n",
              "4                                            0.6521  \n",
              "5                                            0.6538  \n",
              "6                                            0.6489  \n",
              "7                                            0.6542  \n",
              "8                                            0.6543  \n",
              "9                                        -4107.6524  \n",
              "10                      -110485009051377795072.0000  \n",
              "11  -38170860150856579751327727291176321024000.0000  "
            ]
          }
        }
      ],
      "id": "94e59e8a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-14T00:39:07.311679Z",
          "start_time": "2026-02-14T00:39:07.239260Z"
        }
      },
      "source": [
        "# Compare best GD result with closed-form (Problem 3)\n",
        "def fit_closed_form(X, y):\n",
        "    X_design = np.column_stack([np.ones(len(X)), X])\n",
        "    beta, *_ = np.linalg.lstsq(X_design, y, rcond=None)\n",
        "    return beta\n",
        "\n",
        "beta_cf = fit_closed_form(X_train_scaled, y_train)\n",
        "y_train_cf = np.column_stack([np.ones(len(X_train_scaled)), X_train_scaled]) @ beta_cf\n",
        "y_test_cf = np.column_stack([np.ones(len(X_test_scaled)), X_test_scaled]) @ beta_cf\n",
        "\n",
        "print(\"--- Closed-form (optimal) ---\")\n",
        "print(f\"Train MSE: {mean_squared_error(y_train, y_train_cf):,.0f}  R²: {r2_score(y_train, y_train_cf):.4f}\")\n",
        "print(f\"Test  MSE: {mean_squared_error(y_test, y_test_cf):,.0f}  R²: {r2_score(y_test, y_test_cf):.4f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Closed-form (optimal) ---\n",
            "Train MSE: 31,486,167,776  R²: 0.7265\n",
            "Test  MSE: 57,628,154,706  R²: 0.6544\n"
          ]
        }
      ],
      "id": "171ce1ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations:**\n",
        "\n",
        "- **Small learning rate (α = 0.01):** Convergence is very slow. After 10 iterations the model has barely improved. MSE is still huge and R² is negative. By 100 iterations it has improved, but it still has not converged to the optimal solution (R² ≈ 0.39 vs the optimal 0.73). Many more iterations would be needed.\n",
        "\n",
        "- **Moderate learning rate (α = 0.1):** Good balance between speed and stability. By 50 iterations the metrics are close to the closed-form optimum, and by 100 iterations they are nearly identical (R² ≈ 0.726).\n",
        "\n",
        "- **Larger learning rate (α = 0.3):** Converges fastest among the stable rates. It reaches near-optimal MSE/R² in roughly 50 iterations. This shows that a good learning rate can dramatically reduce the number of iterations needed.\n",
        "\n",
        "- **Too-large learning rate (α = 0.5):** The algorithm diverges and the MSE explodes to astronomical values. The step size overshoots the minimum, so each iteration makes the parameters worse. This shows that the learning rate must be small enough relative to the curvature of the loss surface.\n",
        "\n",
        "- **Convergence to the optimal solution:** With a suitable learning rate and enough iterations, gradient descent converges to the same solution as the closed-form method (Problem 3). For example, α = 0.3 at 100 iterations is nearly identical to the closed-form MSE/R².\n",
        "\n",
        "- **Feature standardization is critical:** Without scaling, features have very different magnitudes (e.g., `sqft_lot` ~ 10,000 vs `waterfront` ~ 0/1), which creates an elongated loss surface. A single learning rate cannot work well for all features simultaneously, leading to either divergence or very slow convergence."
      ],
      "id": "77f41698"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}